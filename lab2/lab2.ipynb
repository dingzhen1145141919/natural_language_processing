{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "':' expected after dictionary key (1082576291.py, line 95)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 95\u001b[1;36m\u001b[0m\n\u001b[1;33m    tf_idfs = []\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m ':' expected after dictionary key\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import jieba\n",
    "import thulac\n",
    "import snownlp\n",
    "import pynlpir\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import re\n",
    "\n",
    "# 初始化NLPIR\n",
    "pynlpir.open()\n",
    "\n",
    "# 读取停用词\n",
    "def read_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords = set(f.read().splitlines())\n",
    "    return stopwords\n",
    "\n",
    "# 读取文本文件\n",
    "def read_text_files(directory):\n",
    "    texts = []\n",
    "    for i in range(1, 21):\n",
    "        file_path = os.path.join(directory, f'{i}.txt')\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "# 分词函数\n",
    "def jieba_tokenize(text, stopwords):\n",
    "    words = jieba.lcut(text)\n",
    "    return [word for word in words if word not in stopwords and re.match(r'\\w+', word)]\n",
    "\n",
    "def thulac_tokenize(text, stopwords, thulac_obj):\n",
    "    words_with_pos = thulac_obj.cut(text, text=True).split()\n",
    "    words = [word.split('_')[0] for word in words_with_pos]\n",
    "    return [word for word in words if word not in stopwords and re.match(r'\\w+', word)]\n",
    "\n",
    "def snownlp_tokenize(text, stopwords):\n",
    "    words = snownlp.SnowNLP(text).words\n",
    "    return [word for word in words if word not in stopwords and re.match(r'\\w+', word)]\n",
    "\n",
    "def nlpir_tokenize(text, stopwords):\n",
    "    words = pynlpir.segment(text, pos_tagging=False)\n",
    "    return [word for word in words if word not in stopwords and re.match(r'\\w+', word)]\n",
    "\n",
    "# 计算词频\n",
    "def calculate_tf(word_list):\n",
    "    tf = defaultdict(int)\n",
    "    for word in word_list:\n",
    "        tf[word] += 1\n",
    "    total_words = len(word_list)\n",
    "    return {word: count / total_words for word, count in tf.items()}\n",
    "\n",
    "# 计算IDF\n",
    "def calculate_idf(doc_word_lists):\n",
    "    N = len(doc_word_lists)\n",
    "    idf = defaultdict(float)\n",
    "    for doc in doc_word_lists:\n",
    "        unique_words = set(doc)\n",
    "        for word in unique_words:\n",
    "            idf[word] += 1\n",
    "    for word in idf:\n",
    "        idf[word] = math.log(N / (idf[word] + 1))\n",
    "    return idf\n",
    "\n",
    "# 计算TF-IDF\n",
    "def calculate_tf_idf(tf, idf):\n",
    "    tf_idf = {word: tf[word] * idf[word] for word in tf}\n",
    "    return tf_idf\n",
    "\n",
    "# 绘制词云\n",
    "def plot_wordcloud(word_freq, title):\n",
    "    wordcloud = WordCloud(font_path='simhei.ttf', width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    directory = 'D:\\\\code\\\\natural_language_processing\\\\lab2\\\\dataset'\n",
    "    stopwords_file = 'D:\\\\code\\\\natural_language_processing\\\\lab1\\\\cn_stopwords.txt'\n",
    "    stopwords = read_stopwords(stopwords_file)\n",
    "    texts = read_text_files(directory)\n",
    "\n",
    "    thulac_obj = thulac.thulac()\n",
    "\n",
    "    tokenizers = {\n",
    "        'jieba': jieba_tokenize,\n",
    "        'thulac': thulac_tokenize,\n",
    "        'snownlp': snownlp_tokenize,\n",
    "        'nlpir': nlpir_tokenize\n",
    "    }\n",
    "\n",
    "    all_doc_word_lists = defaultdict(list)\n",
    "    for tokenizer_name, tokenizer_func in tokenizers.items():\n",
    "        for text in texts:\n",
    "            if tokenizer_name == 'thulac':\n",
    "                words = tokenizer_func(text, stopwords, thulac_obj)\n",
    "            else:\n",
    "                words = tokenizer_func(text, stopwords)\n",
    "            all_doc_word_lists[tokenizer_name].append(words)\n",
    "\n",
    "    idfs = {tokenizer_name: calculate_idf(doc_word_lists) for tokenizer_name, doc_word_lists in all_doc_word_lists.items()}\n",
    "\n",
    "    for tokenizer_name, doc_word_lists in all_doc_word_lists.items():\n",
    "        tf_idfs = []\n",
    "        for words in doc_word_lists:\n",
    "            tf = calculate_tf(words)\n",
    "            tf_idf = calculate_tf_idf(tf, idfs[tokenizer_name])\n",
    "            tf_idfs.append(tf_idf)\n",
    "\n",
    "        combined_tf_idf = defaultdict(float)\n",
    "        for tf_idf in tf_idfs:\n",
    "            for word, score in tf_idf.items():\n",
    "                combined_tf_idf[word] += score\n",
    "\n",
    "        # 转换为DataFrame并排序\n",
    "        df = pd.DataFrame(list(combined_tf_idf.items()), columns=['word', 'tfidf'])\n",
    "        df = df.sort_values(by='tfidf', ascending=False).head(20)\n",
    "\n",
    "        # 打印词频最高的20个词\n",
    "        print(f\"Top 20 Words by TF-IDF using {tokenizer_name}:\")\n",
    "        print(df)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        plot_wordcloud(combined_tf_idf, f'TF-IDF Word Cloud using {tokenizer_name}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
